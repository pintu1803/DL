{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpamDetection_1.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pintu1803/DL/blob/main/SpamDetection_1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Spam detection using ML models\n",
        "####By: Pintu, 181co139\n",
        "####Date: 30/01/22"
      ],
      "metadata": {
        "id": "0K9EuvmOCWvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "mount Googledrive\n"
      ],
      "metadata": {
        "id": "048jq3lWWg-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeyajzBzWV9N",
        "outputId": "0db75dff-0c69-455c-90f8-d60ce77a1183"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the dataset"
      ],
      "metadata": {
        "id": "ttrji0eyV-WH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pintu 29/01/22\n",
        "from sklearn import preprocessing\n",
        "import pandas as pd\n",
        "\n",
        "dataset=pd.read_csv('/content/drive/MyDrive/Colab datasets/Spam_dataset.csv', header=0, delimiter=\",\")\n",
        "\n",
        "#remove the missing data.\n",
        "dataset=dataset.dropna()\n",
        "print(\"Head of the dataset =>\\n\",dataset.head()) #dataset.tail()\n",
        "\n",
        "#Category -> Integer\n",
        "#Ham = 0\n",
        "#Spam = 1\n",
        "# le = preprocessing.LabelEncoder()\n",
        "# le.fit(dataset.Category)\n",
        "# dataset['Category'] = le.transform(dataset.Category)\n",
        "\n",
        "spam_cnt=0\n",
        "for i in range(0, len(dataset['Category'])):\n",
        "  if (dataset['Category'][i]=='ham'):\n",
        "    dataset['Category'][i]=\"0\"\n",
        "  else:\n",
        "    dataset['Category'][i]=\"1\"\n",
        "    spam_cnt+=1\n",
        "\n",
        "\n",
        "print(\"\\nTail of dataset after labelling =>\\n\",dataset.tail())\n",
        "print(\"\\nTotal spam texts are =>\", spam_cnt,\" out of \",len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yBgYyQJV5ky",
        "outputId": "77bb2c9a-fe83-40a0-a9a2-e2339fac6ccb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head of the dataset =>\n",
            "   Category                                            Message\n",
            "0      ham  Go until jurong point, crazy.. Available only ...\n",
            "1      ham                      Ok lar... Joking wif u oni...\n",
            "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3      ham  U dun say so early hor... U c already then say...\n",
            "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
            "\n",
            "Tail of dataset after labelling =>\n",
            "      Category                                            Message\n",
            "5567        1  This is the 2nd time we have tried 2 contact u...\n",
            "5568        0               Will Ã¼ b going to esplanade fr home?\n",
            "5569        0  Pity, * was in mood for that. So...any other s...\n",
            "5570        0  The guy did some bitching but I acted like i'd...\n",
            "5571        0                         Rofl. Its true to its name\n",
            "\n",
            "Total spam texts are => 747  out of  5572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PreProcessing\n"
      ],
      "metadata": {
        "id": "x4Zn63h_cxJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pintu 29/01/22\n",
        "#Preprocessing of the dataset step\n",
        "\n",
        "#PUNCTUATION \n",
        "#library that contains punctuation\n",
        "import string\n",
        "string.punctuation\n",
        "#defining the function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
        "    return punctuationfree\n",
        "\n",
        "#TOLOWER\n",
        "#converting text into lower format.\n",
        "def to_lower(text):\n",
        "    lowered_text=text.lower()\n",
        "    return lowered_text\n",
        "\n",
        "#TOKENIZATION\n",
        "#defining function for tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "def tokenization(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "#REMOVE STOPWORDS\n",
        "#importing nlp library\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "#Stop words present in the library\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "print(\"\\nFew english stopwords are: \",stopwords[0:10])\n",
        "\n",
        "#defining the function to remove stopwords from tokenized text\n",
        "def remove_stopwords(text):\n",
        "    output= [i for i in text if i not in stopwords]\n",
        "    return output\n",
        "\n",
        "#STEMMING\n",
        "#importing the Stemming function from nltk library\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "#defining the object for stemming\n",
        "porter_stemmer = PorterStemmer()\n",
        "#defining a function for stemming\n",
        "def stemming(text):\n",
        "    stem_text = [porter_stemmer.stem(word) for word in text]\n",
        "    return stem_text\n",
        "\n",
        "#LEMMATIZATION\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#defining the object for Lemmatization\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "#defining the function for lemmatization\n",
        "def lemmatizer(text):\n",
        "    lemmed_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
        "    return lemmed_text\n",
        "\n",
        "\n",
        "##PRE-PROCESSING##\n",
        "def text_preprocessing(text):\n",
        "    punctuation_free=remove_punctuation(text)\n",
        "    lower_text=to_lower(punctuation_free)\n",
        "    tokenized_text=tokenization(lower_text)\n",
        "    stopwords_free=remove_stopwords(tokenized_text)\n",
        "    stemmed_text=stemming(stopwords_free)\n",
        "    lemmatized_text=lemmatizer(stemmed_text)\n",
        "    return [punctuation_free,lower_text,tokenized_text,stopwords_free,stemmed_text,lemmatized_text]\n",
        "\n",
        "#Apply pre-processing function.\n",
        "for i in range(0,len(dataset['Message'])):\n",
        "  dataset['Message'][i]=text_preprocessing(dataset['Message'][i])[5]\n",
        "\n",
        "#Many other steps include: URL removal, HTML tags removal, Rare words removal, Frequent words removal, Spelling checking, and many more.\n",
        "#But we are not going for further pre-processing cause we need to detect spams and spam contains URL."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyTlMdgjWOIJ",
        "outputId": "2dec0c69-d2dc-4b58-b256-fe8612e2cf4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "\n",
            "Few english stopwords are:  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify pre-processing"
      ],
      "metadata": {
        "id": "VyyzZC6PYH0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pintu 29/01/22\n",
        "#verify.\n",
        "\n",
        "text=\"Let's See; CONNECTION Misunderstanding swimming; ASSure the Authentication: of Working of tExt Pre-processing\"\n",
        "[punctuation_free,lower_text,tokenized_text,stopwords_free,stemmed_text,lemmatized_text]=text_preprocessing(text)\n",
        "print(\"Remove punctuation=> \",punctuation_free)\n",
        "print(\"Lowered text      => \",lower_text)\n",
        "print(\"Tokens separated  => \",tokenized_text)\n",
        "print(\"Remove stopwords  => \",stopwords_free)\n",
        "print(\"Stemming          => \",stemmed_text)\n",
        "print(\"Lemmatizer        => \",lemmatized_text)\n",
        "\n",
        "print(\"\\nVerify the proper preprocessing:\\n\",dataset['Message'][0:7])\n",
        "\n",
        "#success msg.\n",
        "print(\"\\nAll texts have been preprocessed successfully!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcO8eoE-YHCh",
        "outputId": "ebc4959a-385c-4e6e-c137-4804998bd9ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remove punctuation=>  Lets See CONNECTION Misunderstanding swimming ASSure the Authentication of Working of tExt Preprocessing\n",
            "Lowered text      =>  lets see connection misunderstanding swimming assure the authentication of working of text preprocessing\n",
            "Tokens separated  =>  ['lets', 'see', 'connection', 'misunderstanding', 'swimming', 'assure', 'the', 'authentication', 'of', 'working', 'of', 'text', 'preprocessing']\n",
            "Remove stopwords  =>  ['lets', 'see', 'connection', 'misunderstanding', 'swimming', 'assure', 'authentication', 'working', 'text', 'preprocessing']\n",
            "Stemming          =>  ['let', 'see', 'connect', 'misunderstand', 'swim', 'assur', 'authent', 'work', 'text', 'preprocess']\n",
            "Lemmatizer        =>  ['let', 'see', 'connect', 'misunderstand', 'swim', 'assur', 'authent', 'work', 'text', 'preprocess']\n",
            "\n",
            "Verify the proper preprocessing:\n",
            " 0    [go, jurong, point, crazi, avail, bugi, n, gre...\n",
            "1                         [ok, lar, joke, wif, u, oni]\n",
            "2    [free, entri, 2, wkli, comp, win, fa, cup, fin...\n",
            "3        [u, dun, say, earli, hor, u, c, alreadi, say]\n",
            "4    [nah, dont, think, goe, usf, live, around, tho...\n",
            "5    [freemsg, hey, darl, 3, week, word, back, id, ...\n",
            "6    [even, brother, like, speak, treat, like, aid,...\n",
            "Name: Message, dtype: object\n",
            "\n",
            "All texts have been preprocessed successfully!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Vocabulary"
      ],
      "metadata": {
        "id": "CodhyaMVgLWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pintu 30/01/22\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer                    \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "x=dataset['Message']\n",
        "y=dataset['Category']\n",
        "\n",
        "#create the dict.\n",
        "tokenizer = Tokenizer(num_words=8000)\n",
        "tokenizer.fit_on_texts(x)\n",
        "\n",
        "#number of unique words in dict.\n",
        "print(\"Number of unique words in dictionary=\",len(tokenizer.word_index))\n",
        "\n",
        "#replace words with their index in vocab.\n",
        "x_train = tokenizer.texts_to_sequences(x)\n",
        "\n",
        "\n",
        "# Adding 1 because of  reserved 0 index\n",
        "vocab_size = len(tokenizer.word_index) + 1  \n",
        "\n",
        "#size of random text in training set.\n",
        "print(\"Length of random text=\")\n",
        "for i in range(10):\n",
        "  print(\"Length of \",i+1,\" text is => \",len(x_train[i]))\n",
        "\n",
        "#Maximum length of each text\n",
        "maxlen = 30\n",
        "\n",
        "#pad the short text and truncate longer texts.\n",
        "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV6RDKhOOnSP",
        "outputId": "7da084f3-9041-45ca-b207-cef30d29922a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in dictionary= 8145\n",
            "Length of random text=\n",
            "Length of  1  text is =>  16\n",
            "Length of  2  text is =>  6\n",
            "Length of  3  text is =>  23\n",
            "Length of  4  text is =>  9\n",
            "Length of  5  text is =>  8\n",
            "Length of  6  text is =>  19\n",
            "Length of  7  text is =>  8\n",
            "Length of  8  text is =>  16\n",
            "Length of  9  text is =>  18\n",
            "Length of  10  text is =>  18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm that text is converted into vector"
      ],
      "metadata": {
        "id": "V2Pnhfm5hVcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pintu 30/01/22\n",
        "print(\"First tweet=> \",x[0])\n",
        "print(\"Text to seq=> \",x_train[1])\n",
        "print(\"\\nVerification successful!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O8yhi2AQJbJ",
        "outputId": "e78292d6-f5b4-4a2c-fc3e-ad7de1686d87"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First tweet=>  ['go', 'jurong', 'point', 'crazi', 'avail', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amor', 'wat']\n",
            "Text to seq=>  [  11  249  560  366    1 1596    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "\n",
            "Verification successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE: Synthetic Minority Oversampling TEchnique"
      ],
      "metadata": {
        "id": "1KwlP5caQkda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pintu 30/01/22\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "#print(imblearn.__version__)\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "msg,y= x_train, dataset['Category'].to_numpy()\n",
        "print(\"Type of message column and class label => \",type(msg),type(y))\n",
        "count = Counter(y)\n",
        "print(\"\\nInitial Count of each class is =>\",count)\n",
        "\n",
        "# over = SMOTE(sampling_strategy=0.2) #sampling_strategy argument tells that minority = 10% of majority.\n",
        "# X, y = over.fit_resample(X, y)\n",
        "# under = RandomUnderSampler(sampling_strategy=0.5) #sampling_strategy argument tells that majority = 1/0.5 of minority.\n",
        "# X, y = under.fit_resample(X, y)\n",
        "\n",
        "#Using Pipeline instead of doing separately.\n",
        "over = SMOTE(sampling_strategy=0.2)\n",
        "under = RandomUnderSampler(sampling_strategy=0.5)\n",
        "steps_in_order = [('o', over), ('u', under)]\n",
        "pipeline = Pipeline(steps=steps_in_order)\n",
        "\n",
        "# transform the dataset\n",
        "X, y = pipeline.fit_resample(msg, y)\n",
        "# summarize the new class distribution\n",
        "counter = Counter(y)\n",
        "print(\"\\nNew count after SMOTE of both class =>\",counter)\n",
        "print(\"\\nSize of X and y => \",len(X),len(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V66VNaXsQkCe",
        "outputId": "38042c0d-58ee-4758-a064-96054eaf17e1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of message column and class label =>  <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "\n",
            "Initial Count of each class is => Counter({'0': 4825, '1': 747})\n",
            "\n",
            "New count after SMOTE of both class => Counter({'0': 1930, '1': 965})\n",
            "\n",
            "Size of X and y =>  2895 2895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download zip glove file"
      ],
      "metadata": {
        "id": "opIXMwzdhh0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqYbN3pRTkk_",
        "outputId": "72d883dd-9f28-416f-c2e2-926cf235c30c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-30 06:06:22--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-01-30 06:06:22--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-01-30 06:06:23--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: âglove.6B.zipâ\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.17MB/s    in 2m 40s  \n",
            "\n",
            "2022-01-30 06:09:02 (5.15 MB/s) - âglove.6B.zipâ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip glove file"
      ],
      "metadata": {
        "id": "zvtfxeO9hprL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove*.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7q1sN_-To0q",
        "outputId": "57562e53-9563-4e14-d35f-58ded65e4261"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create embeddings of vocab."
      ],
      "metadata": {
        "id": "qWAgKLqr4WS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pintu 30/01/22\n",
        "import numpy as np\n",
        "\n",
        "#vocab: 'the': 1, mapping of words with integers in seq. 1,2,3..\n",
        "#embedding: 1->dense vector\n",
        "\n",
        "def embedding_for_vocab(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  \n",
        "    # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix_vocab = np.zeros((vocab_size, embedding_dim))\n",
        "  \n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix_vocab[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix_vocab\n",
        "\n",
        "#matrix for vocab: word_index\n",
        "embedding_dim = 50\n",
        "embedding_matrix_vocab = embedding_for_vocab('/content/glove.6B.50d.txt',tokenizer.word_index,embedding_dim)\n"
      ],
      "metadata": {
        "id": "s_JXnoME4VcI"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify the vocabulary and embedding matrix."
      ],
      "metadata": {
        "id": "jz8GWlVE5N3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pintu 30/01/22\n",
        "\n",
        "print(\"Type of vocabulary => \",type(tokenizer.word_index))\n",
        "print(\"\\nVocabulary glance => \",)\n",
        "stop=0\n",
        "\n",
        "#see the dict.\n",
        "for word,index in tokenizer.word_index.items():\n",
        "  stop+=1\n",
        "  if(stop==10):\n",
        "    break\n",
        "  print(index,\" => \",word)\n",
        "  \n",
        "#dense vector.\n",
        "print(\"\\nDense vector of first word in dict => \\n\",embedding_matrix_vocab[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAzpXnvQ5GXn",
        "outputId": "b4626592-07a1-47b9-9448-aae19a74194f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of vocabulary =>  <class 'dict'>\n",
            "\n",
            "Vocabulary glance => \n",
            "1  =>  u\n",
            "2  =>  call\n",
            "3  =>  2\n",
            "4  =>  im\n",
            "5  =>  go\n",
            "6  =>  get\n",
            "7  =>  ur\n",
            "8  =>  come\n",
            "9  =>  4\n",
            "\n",
            "Dense vector of first word in dict => \n",
            " [-0.25676     0.8549      1.10029995  0.95362997  0.36585    -1.30289996\n",
            "  1.07539999 -0.18460999 -0.67673999  0.37637001 -0.029637    0.51697999\n",
            " -0.19248    -0.41863    -0.71144003  0.12564    -0.42965001  0.61456001\n",
            "  0.41819     0.27605999 -0.48635    -0.32585001  0.67747998  0.15916\n",
            "  0.35051    -0.29392999 -0.80439001 -0.15939     0.012475   -0.58403999\n",
            "  2.13529992 -0.1547     -0.57389998  1.45220006  0.6124     -0.68752003\n",
            "  1.28390002 -0.54631001 -0.35736999  0.57323003  0.35460001 -0.37465\n",
            " -0.74628001 -0.074561   -0.48471001  0.067343   -0.039338   -0.22177\n",
            "  0.099708    0.55553001]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create embedding for all tweet texts."
      ],
      "metadata": {
        "id": "8VQLWEV14g--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pintu 30/01/22\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#tweet_text: [23, 0, 34, ..., 35] tweet converted into seq.\n",
        "#goal: [[dense vector for 23], ..., [dense vecor for 35]]\n",
        "def embedded_tweet_text(embedding_matrix_vocab, tweet_text):\n",
        "    matrix_size = len(tweet_text)   \n",
        "    \n",
        "    tweet_text_matrix = np.zeros((matrix_size, embedding_dim))\n",
        "    \n",
        "    #traverse pad_seq of each tweet text\n",
        "    for i in range(0,matrix_size):\n",
        "      index=tweet_text[i]\n",
        "      if(index==0):\n",
        "        continue\n",
        "      else:\n",
        "        tweet_text_matrix[i]=embedding_matrix_vocab[index]\n",
        "    return tweet_text_matrix\n",
        "\n",
        "#define zero matrix.\n",
        "n=len(X)\n",
        "x1=np.zeros((n, 30, 50))\n",
        "\n",
        "#populate matrix.\n",
        "for i in range(0,n):\n",
        "  x1[i]=embedded_tweet_text(embedding_matrix_vocab, X[i])\n",
        "\n",
        "#ML model don't accept input in 3D shape\n",
        "#so reshape 3D into 2D: 1D array per tweet.\n",
        "x2=x1.reshape(len(X),-1)\n",
        "print(\"Shape of x2 => \",x2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUNt5zL94gWj",
        "outputId": "0ce082fa-a8fd-4941-bfcb-d87bd1bc59a1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x2 =>  (2895, 1500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifying whole embedding."
      ],
      "metadata": {
        "id": "ZDnAnHkB6vnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#x <- preprocessed\n",
        "#x_train <- padded seq\n",
        "#X <- smoted\n",
        "#x1 <- final embedding\n",
        "\n",
        "print(\"First tweet as text =>\\n\",dataset['Message'][1])\n",
        "print(\"First tweet: text->seq =>\\n\",x_train[1])\n",
        "print(\"Dense vector for 1st word of above tweet =>\\n\",embedding_matrix_vocab[22])\n",
        "print(\"First tweet -> 2D matrix of dense vectors =>\\n\",x1[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoHD7ZKU6IEb",
        "outputId": "52a508e0-20da-4085-cb4c-f22931318dca"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First tweet as text =>\n",
            " ['ok', 'lar', 'joke', 'wif', 'u', 'oni']\n",
            "First tweet: text->seq =>\n",
            " [  11  249  560  366    1 1596    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "Dense vector for 1st word of above tweet =>\n",
            " [-3.55859995e-01  5.21300018e-01 -6.10700011e-01 -3.01310003e-01\n",
            "  9.48620021e-01 -3.15389991e-01 -5.98309994e-01  1.21880002e-01\n",
            " -3.19430009e-02  5.56949973e-01 -1.06210001e-01  6.33989990e-01\n",
            " -4.73399997e-01 -7.58949965e-02  3.82470012e-01  8.15690011e-02\n",
            "  8.22139978e-01  2.22200006e-01 -8.37639999e-03 -7.66200006e-01\n",
            " -5.62529981e-01  6.17590010e-01  2.02920005e-01 -4.85979989e-02\n",
            "  8.78149986e-01 -1.65489995e+00 -7.74179995e-01  1.54349998e-01\n",
            "  9.48230028e-01 -3.95200014e-01  3.73020005e+00  8.28549981e-01\n",
            " -1.41039997e-01  1.63950007e-02  2.11150005e-01 -3.60849984e-02\n",
            " -1.55870005e-01  8.65830004e-01  2.63090014e-01 -7.10150003e-01\n",
            " -3.67700011e-02  1.82819995e-03 -1.77039996e-01  2.70319998e-01\n",
            "  1.10260002e-01  1.41330004e-01 -5.73219992e-02  2.72069991e-01\n",
            "  3.13050002e-01  9.27709997e-01]\n",
            "First tweet -> 2D matrix of dense vectors =>\n",
            " [[-0.35585999  0.52130002 -0.61070001 ...  0.27206999  0.31305\n",
            "   0.92771   ]\n",
            " [ 0.13771001  0.15251    -0.28597999 ... -0.1062      0.080406\n",
            "   0.11109   ]\n",
            " [-1.35350001  0.90126002  0.026341   ... -1.58410001 -0.93278003\n",
            "   0.58598   ]\n",
            " ...\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Models: k-fold cross validation using accuracy as metric."
      ],
      "metadata": {
        "id": "C1qwIGh3UXO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pintu 30/01/22\n",
        "\n",
        "#from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# X and y are being SMOTE\n",
        "\n",
        "folds = range(3,10)\n",
        "\n",
        "# define the model to be evaluate\n",
        "\n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = list()\n",
        "\tmodels.append(LogisticRegression())\n",
        "\tmodels.append(SGDClassifier())\n",
        "\tmodels.append(KNeighborsClassifier())\n",
        "\tmodels.append(DecisionTreeClassifier())\n",
        "\tmodels.append(SVC())\n",
        "\tmodels.append(GaussianNB())\n",
        "\tmodels.append(RandomForestClassifier())\n",
        "\tmodels.append(GradientBoostingClassifier())\n",
        "\treturn models\n",
        "\n",
        "models = get_models()\n",
        "# evaluate each k value for a model\n",
        "def evaluate_model(model):\n",
        "  for k in folds:\n",
        "    cross_validation = KFold(n_splits=k, shuffle=True, random_state=1)\n",
        "    scores = cross_val_score(model, x2, y, scoring='accuracy', cv=cross_validation, n_jobs=-1)\n",
        "    print(\"Average of the accuracy for \",k,\"folds => \",mean(scores))\n",
        "\n",
        "# evaluate each model\n",
        "for m in models:\n",
        "  model=m\n",
        "  print(\"\\nSummary of model:\",model)\n",
        "  evaluate_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOriH0WIGk5x",
        "outputId": "30358bf2-e8e2-4ec1-fccd-a1f8c7649c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary of model: LogisticRegression()\n",
            "Average of the accuracy for  3 folds =>  0.8818652849740932\n",
            "Average of the accuracy for  4 folds =>  0.8853132665459297\n",
            "Average of the accuracy for  5 folds =>  0.8870466321243523\n",
            "Average of the accuracy for  6 folds =>  0.8790974745782613\n",
            "Average of the accuracy for  7 folds =>  0.8859971893449102\n",
            "Average of the accuracy for  8 folds =>  0.8853198986853583\n",
            "Average of the accuracy for  9 folds =>  0.8853060118805751\n",
            "\n",
            "Summary of model: SGDClassifier()\n",
            "Average of the accuracy for  3 folds =>  0.865630397236615\n",
            "Average of the accuracy for  4 folds =>  0.8670030871980622\n",
            "Average of the accuracy for  5 folds =>  0.8735751295336789\n",
            "Average of the accuracy for  6 folds =>  0.8721911520035279\n",
            "Average of the accuracy for  7 folds =>  0.8721803631794157\n",
            "Average of the accuracy for  8 folds =>  0.8718463522137709\n",
            "Average of the accuracy for  9 folds =>  0.8787379415172994\n",
            "\n",
            "Summary of model: KNeighborsClassifier()\n",
            "Average of the accuracy for  3 folds =>  0.7343696027633851\n",
            "Average of the accuracy for  4 folds =>  0.7416224410261113\n",
            "Average of the accuracy for  5 folds =>  0.7450777202072538\n",
            "Average of the accuracy for  6 folds =>  0.7426555443874586\n",
            "Average of the accuracy for  7 folds =>  0.744387462673598\n",
            "Average of the accuracy for  8 folds =>  0.747155308305658\n",
            "Average of the accuracy for  9 folds =>  0.7478280219036009\n",
            "\n",
            "Summary of model: DecisionTreeClassifier()\n",
            "Average of the accuracy for  3 folds =>  0.8013816925734023\n",
            "Average of the accuracy for  4 folds =>  0.7989572109763645\n",
            "Average of the accuracy for  5 folds =>  0.7968911917098446\n",
            "Average of the accuracy for  6 folds =>  0.8055297830239199\n",
            "Average of the accuracy for  7 folds =>  0.8096675172156802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Program ended!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtixl6_zV3Fn",
        "outputId": "dca32379-992e-4c33-ccc0-c032d0921d84"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Program ended!\n"
          ]
        }
      ]
    }
  ]
}